{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Collaborative-Filtering-for-medical-history/blob/master/LSTM_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dflUNFEBKaBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "print(\"Tensorflow version: \",tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dzK5gw0PZZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63shfdreVapA",
        "colab_type": "code",
        "outputId": "59d7bfc4-8432-4763-d147-dfb6a8f1d167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "path=os.getcwd()\n",
        "\n",
        "text=open(path+'/Iliad_v3.txt','rb').read().decode(encoding='utf-8')\n",
        "print(\"Text is {} characters long\".format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text is 886809 characters long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oalGpKWMWuNR",
        "colab_type": "code",
        "outputId": "6b9450f6-0ea3-4def-9e55-704dd2a834a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words=[w for w in text.split(' ') if w.strip()!='' or w=='\\n']\n",
        "print(\"Text is {} words long\".format(len(words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text is 153260 words long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tqhNC3wXEz1",
        "colab_type": "code",
        "outputId": "ac89995c-583d-49c4-c9fb-fde8dd6b1ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "vocab=sorted(set(text))\n",
        "print(\"There are {} unique characters\".format(len(vocab)))\n",
        "char2int={c:i for i,c in enumerate(vocab)}\n",
        "int2char=np.array(vocab)\n",
        "print(\"Vector:\\n\")\n",
        "for char,_ in zip(char2int,range(len(vocab))):\n",
        "  print(' {:4s}: {:3d},'.format(repr(char), char2int[char]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 34 unique characters\n",
            "Vector:\n",
            "\n",
            " '\\n':   0,\n",
            " ' ' :   1,\n",
            " '!' :   2,\n",
            " \"'\" :   3,\n",
            " ',' :   4,\n",
            " '-' :   5,\n",
            " '.' :   6,\n",
            " '?' :   7,\n",
            " 'a' :   8,\n",
            " 'b' :   9,\n",
            " 'c' :  10,\n",
            " 'd' :  11,\n",
            " 'e' :  12,\n",
            " 'f' :  13,\n",
            " 'g' :  14,\n",
            " 'h' :  15,\n",
            " 'i' :  16,\n",
            " 'j' :  17,\n",
            " 'k' :  18,\n",
            " 'l' :  19,\n",
            " 'm' :  20,\n",
            " 'n' :  21,\n",
            " 'o' :  22,\n",
            " 'p' :  23,\n",
            " 'q' :  24,\n",
            " 'r' :  25,\n",
            " 's' :  26,\n",
            " 't' :  27,\n",
            " 'u' :  28,\n",
            " 'v' :  29,\n",
            " 'w' :  30,\n",
            " 'x' :  31,\n",
            " 'y' :  32,\n",
            " 'z' :  33,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSfwTddbxsAW",
        "colab_type": "text"
      },
      "source": [
        "Each characted mapped as a no."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLbNb2WKxLXH",
        "colab_type": "code",
        "outputId": "63c064b4-9f4e-4a01-8f6b-2f144a130062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "print ('{}\\n mapped to integers:\\n {}'.format(repr(text[:100]), text_as_int[:100]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  achilles' wrath, to greece the direful spring\\n  of woes unnumber'd, heavenly goddess, sing!\\n  that\"\n",
            " mapped to integers:\n",
            " [ 1  1  8 10 15 16 19 19 12 26  3  1 30 25  8 27 15  4  1 27 22  1 14 25\n",
            " 12 12 10 12  1 27 15 12  1 11 16 25 12 13 28 19  1 26 23 25 16 21 14  0\n",
            "  1  1 22 13  1 30 22 12 26  1 28 21 21 28 20  9 12 25  3 11  4  1 15 12\n",
            "  8 29 12 21 19 32  1 14 22 11 11 12 26 26  4  1 26 16 21 14  2  0  1  1\n",
            " 27 15  8 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MC4ZvBBxi-d",
        "colab_type": "text"
      },
      "source": [
        "Creating training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqu_kamxiV1",
        "colab_type": "code",
        "outputId": "b90c73ea-2730-40e5-9d1b-73acdc75dd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tr_text = text_as_int[:704000] \n",
        "val_text = text_as_int[704000:] \n",
        "print(\"Total size: {}, Train size : {}, Test size:{}\" \\\n",
        ".format(text_as_int.shape, tr_text.shape, val_text.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total size: (886809,), Train size : (704000,), Test size:(182809,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW0wfYxUyMmm",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kz1ltYzyPCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=64\n",
        "buffer_size=10000\n",
        "embedding_dim=256\n",
        "epochs=50\n",
        "seq_length=200\n",
        "examples_per_epoch=len(text)//seq_length\n",
        "rnn_units=1024\n",
        "vocab_size=len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrvQjIp3pHc",
        "colab_type": "text"
      },
      "source": [
        "Slicing the dataset in batchsizes, shuffling and mapping input to respective target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqUUdSa1Sgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_char_dataset=tf.data.Dataset.from_tensor_slices(tr_text) #what does tensor slices do\n",
        "val_char_dataset=tf.data.Dataset.from_tensor_slices(val_text) \n",
        "tr_sequences=tr_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_sequences=val_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "  input_text=chunk[:-1]\n",
        "  target_text=chunk[1:]\n",
        "  return input_text,target_text\n",
        "\n",
        "tr_dataset=tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "val_dataset=val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiBRjVmG41-0",
        "colab_type": "text"
      },
      "source": [
        "Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk4IhsH741rX",
        "colab_type": "code",
        "outputId": "7616c9d1-b85c-496f-b98a-c22aefe45cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model=build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f33e00b0898>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzqJBElp6gIS",
        "colab_type": "text"
      },
      "source": [
        "##Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQOTNe_F6iqg",
        "colab_type": "code",
        "outputId": "f1354976-0d79-47b4-f242-7f810e732639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.summary()\n",
        "for input_example_batch,target_example_batch in tr_dataset.take(1):\n",
        "  example_batch_predictions=model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)\n",
        "\n",
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)\n",
        "\n",
        "example_batch_loss=loss(target_example_batch,example_batch_predictions)\n",
        "print(\"Loss: \",example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           8704      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (64, None, 256)           0         \n",
            "_________________________________________________________________\n",
            "unified_lstm (UnifiedLSTM)   (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 34)            34850     \n",
            "=================================================================\n",
            "Total params: 5,290,530\n",
            "Trainable params: 5,290,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(64, 200, 34)\n",
            "Loss:  3.5278356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6owpn1YtT1_Q",
        "colab_type": "code",
        "outputId": "c579b4a3-406b-49ea-c803-d45bb2c4c8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "input_example_batch[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=482, shape=(200,), dtype=int32, numpy=\n",
              "array([26, 27,  1,  8, 21, 11,  1, 21, 22,  9, 19, 12, 26, 27,  1, 22, 13,\n",
              "        1, 27, 15, 12,  1, 14, 25, 12, 10, 16,  8, 21,  1, 27, 25,  8, 16,\n",
              "       21,  1,  0,  1,  1, 23, 12, 25, 20, 16, 27,  1, 21, 22, 27,  1, 27,\n",
              "       15, 12, 26, 12,  1, 27, 22,  1, 26, 28, 12,  4,  1,  8, 21, 11,  1,\n",
              "       26, 28, 12,  1, 16, 21,  1, 29,  8, 16, 21,  2,  0,  1,  1, 19, 12,\n",
              "       27,  1, 20, 12,  1,  1, 20, 32,  1, 26, 22, 21,  1,  1,  8, 21,  1,\n",
              "        8, 21, 10, 16, 12, 21, 27,  1, 13,  8, 10, 27,  1, 28, 21, 13, 22,\n",
              "       19, 11,  4,  0,  1,  1,  8,  1, 14, 25, 12,  8, 27,  1, 12, 31,  8,\n",
              "       20, 23, 19, 12,  1, 11, 25,  8, 30, 21,  1, 13, 25, 22, 20,  1, 27,\n",
              "       16, 20, 12, 26,  1, 22, 13,  1, 22, 19, 11,  1,  0,  1,  1, 15, 12,\n",
              "        8, 25,  1, 30, 15,  8, 27,  1, 22, 28, 25,  1, 13,  8, 27, 15, 12,\n",
              "       25, 26,  1, 30, 12, 25, 12,  4,  1,  8, 21, 11,  1], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsrKqrNgMqpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "patience=10\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQkAftLuNICC",
        "colab_type": "code",
        "outputId": "af9719e0-4f04-424e-b437-b7a8676d1e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "checkpoint_dir='./checkpoints'+datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
        "checkpoint_prefix=os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                                       save_weights_only=True)\n",
        "history = model.fit(tr_dataset, epochs=epochs, callbacks=[checkpoint_callback, early_stop] , validation_data=val_dataset)\n",
        "print (\"Training stopped as there was no improvement after {} epochs\".format(patience))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "54/54 [==============================] - 20s 368ms/step - loss: 2.8847 - val_loss: 2.5216\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 18s 341ms/step - loss: 2.2672 - val_loss: 2.1183\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 19s 344ms/step - loss: 2.0279 - val_loss: 1.9331\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 19s 349ms/step - loss: 1.8699 - val_loss: 1.7988\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 19s 348ms/step - loss: 1.7408 - val_loss: 1.6870\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 19s 351ms/step - loss: 1.6318 - val_loss: 1.5947\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 19s 351ms/step - loss: 1.5427 - val_loss: 1.5228\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.4669 - val_loss: 1.4648\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 19s 350ms/step - loss: 1.4062 - val_loss: 1.4202\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.3566 - val_loss: 1.3889\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 19s 351ms/step - loss: 1.3130 - val_loss: 1.3534\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.2750 - val_loss: 1.3317\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.2410 - val_loss: 1.3152\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 1.2086 - val_loss: 1.3053\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.1799 - val_loss: 1.2967\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.1513 - val_loss: 1.2943\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.1257 - val_loss: 1.2960\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 1.1010 - val_loss: 1.2974\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.0726 - val_loss: 1.3008\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 1.0439 - val_loss: 1.3053\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 1.0171 - val_loss: 1.3146\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 0.9909 - val_loss: 1.3265\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 19s 352ms/step - loss: 0.9631 - val_loss: 1.3357\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 0.9364 - val_loss: 1.3498\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 0.9124 - val_loss: 1.3670\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 19s 353ms/step - loss: 0.8825 - val_loss: 1.3839\n",
            "Training stopped as there was no improvement after 10 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jscWY4sS2_2",
        "colab_type": "code",
        "outputId": "bf02688d-0696-451a-8679-9065f5940451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \n",
        "model.build(tf.TensorShape([1, None]))\n",
        "def generate_text(model, start_string):\n",
        "    \n",
        "    print('Generating with seed: \"' + start_string + '\"')\n",
        "  \n",
        "    num_generate =500\n",
        "    input_eval = [char2int[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        #print(input_eval.shape)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
        "        predicted_id_shaped=tf.expand_dims([predicted_id], 0)\n",
        "        input_eval=tf.concat([input_eval,predicted_id_shaped],1)\n",
        "        #print(input_eval)\n",
        "        #input_eval = tf.expand_dims([predicted_id], 1) #changes the shape of input val e.g converting tensor of size 2 -> [2,1]\n",
        "        text_generated.append(int2char[predicted_id])\n",
        "        #print(text_generated)\n",
        "    return (start_string + ''.join(text_generated))\n",
        "start_string=\"friend, \\\n",
        "  to diomed return from ancy o'er the plain \\\n",
        "  behold the dreadful valour moves to meet,\\\n",
        "  let his broad belt the spackful dares becease.\\\n",
        "  furious are than adorn to the nave \\\n",
        "  and fear'd with tyrant  and his heavenly stream \\\n",
        "  on high grieftes his ships address'd his main.\\\n",
        "  jove speak his chariot, and a svain around,\\\n",
        "  lay gave the fate  the fate of war to wait,\\\n",
        "  who kneess no more, haspenon valia's race,\\\n",
        "  the god of tyrants, shall lead it oeling spoils,\\\n",
        "  in secret dreamful rocks \"\n",
        "print(generate_text(model, start_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f338a96d080>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "Generating with seed: \"friend,   to diomed return from ancy o'er the plain   behold the dreadful valour moves to meet,  let his broad belt the spackful dares becease.  furious are than adorn to the nave   and fear'd with tyrant  and his heavenly stream   on high grieftes his ships address'd his main.  jove speak his chariot, and a svain around,  lay gave the fate  the fate of war to wait,  who kneess no more, haspenon valia's race,  the god of tyrants, shall lead it oeling spoils,  in secret dreamful rocks \"\n",
            "friend,   to diomed return from ancy o'er the plain   behold the dreadful valour moves to meet,  let his broad belt the spackful dares becease.  furious are than adorn to the nave   and fear'd with tyrant  and his heavenly stream   on high grieftes his ships address'd his main.  jove speak his chariot, and a svain around,  lay gave the fate  the fate of war to wait,  who kneess no more, haspenon valia's race,  the god of tyrants, shall lead it oeling spoils,  in secret dreamful rocks to end\n",
            "  the dark abross her lifted hand have stone,\n",
            "  or where dycad fell greece to sens he lay,\n",
            "  trans armm be mind the lighten from the sand \n",
            "  the hardy terrors in their chariots brought \n",
            "  so when achilles rise \n",
            "  once more the living force, and calms his gore-\n",
            "  if e'er it pease  from ilion's towers above?\n",
            "  sand power whom eyes, 'tis his should bear,\n",
            "  pour fate and fate of phroguhes in gift demands \n",
            "   since felt the gods  but if ulysses wish ,   \n",
            "  so shakes  the mangle orger sounds,\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FccnH_WBc0co",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/generating-text-with-tensorflow-2-0-6a65c7bdc568"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3mzGA7mc1De",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}